{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "810e307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd0857",
   "metadata": {},
   "source": [
    "Building a Language Model Application\n",
    "\n",
    "LLMS: Get preicctions from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a506a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "984ced80",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0.99) #temp is the randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d35bdd43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Rome, Italy\n",
      "2. Naples, Italy\n",
      "3. Bologna, Italy\n",
      "4. Florence, Italy\n",
      "5. Venice, Italy\n"
     ]
    }
   ],
   "source": [
    "text = \"What are the 5 vacation destinations for someone who likes to eat pasta??\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750a5a1",
   "metadata": {},
   "source": [
    "Prompt Templates: Manage prompts for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26fd4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "033bc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"food\"],\n",
    "    template = \"What are 5 vacation destinations for someone who likes to eat {food} ?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ea626ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 vacation destinations for someone who likes to eat dessert ?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(food = \"dessert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2af6b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Paris, France\n",
      "2. Rome, Italy\n",
      "3. Tokyo, Japan\n",
      "4. Austin, Texas\n",
      "5. New York City, New York\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(food = \"dessert\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4255c",
   "metadata": {},
   "source": [
    "Chains: Combine LLMs and prompts in multi-step workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c380d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "909b8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0.99)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"food\"],\n",
    "    template = \"What are 5 vacation destinations for someone who likes to eat {food} ?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73c40d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8cee7ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Hawaii - Here you can enjoy a variety of exotic fruits from fresh pineapple, mango, papaya, and guava.\n",
      "\n",
      "2. India - India is known for its abundance of various fruits, such as lychees, mangos, grapes, papayas, and bananas.\n",
      "\n",
      "3. Costa Rica - Costa Rica's climate has allowed for the growth of some delicious tropical fruits like bananas, pineapples, and mangoes.\n",
      "\n",
      "4. Thailand - Thailand offers an array of fruit, with some of the most popular finds being coconuts, durian, rambutan, and dragonfruit.\n",
      "\n",
      "5. South Africa - South Africa is one of the best places to visit if you're looking for some delicious and unique fruits, such as guavas, loquats, and soursops.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"fruit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a626636",
   "metadata": {},
   "source": [
    "Agents: Dynamically call chains based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "888b8a2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in c:\\users\\shubham\\anaconda3\\envs\\langch\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shubham\\anaconda3\\envs\\langch\\lib\\site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shubham\\anaconda3\\envs\\langch\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubham\\anaconda3\\envs\\langch\\lib\\site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubham\\anaconda3\\envs\\langch\\lib\\site-packages (from requests->google-search-results) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubham\\anaconda3\\envs\\langch\\lib\\site-packages (from requests->google-search-results) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "02514165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d009e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "llm = OpenAI(temperature = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ba9caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in some tools to use\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"45793e5dbf981f0c52148284a44bb250e5ea6f4ae1fc16c93f39cf7f7c7a228d\"\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0ec3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, let's initialize an agent with:\n",
    "# 1. The tools\n",
    "# 2. The language model\n",
    "# 3. The type of the agent we want to use\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "df9fa5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the current Prime Minister of India is and then find out the name of his wife.\n",
      "Action: Search\n",
      "Action Input: \"Current Prime Minister of India\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNarendra Modi\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out the name of his wife\n",
      "Action: Search\n",
      "Action Input: \"Narendra Modi's wife\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mJashodaben Modi\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Jashodaben Modi is the wife of the current Prime Minister of India, Narendra Modi.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jashodaben Modi is the wife of the current Prime Minister of India, Narendra Modi.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets test it out\n",
    "agent.run(\"Who is the current Prime Minister of India? What is the name of his wife?\")#giving wrong result when i asked about the prime numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920ac5c",
   "metadata": {},
   "source": [
    "Memory : Add state to chains and agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "41d9df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d56b107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0.9)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec759801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello! It's nice to meet you. What can I do for you?\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9814ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. What can I do for you?\n",
      "Human: I'm doing very well: Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! It's always nice to converse with someone new. What brings you here today?\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing very well: Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61ce5cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. What can I do for you?\n",
      "Human: I'm doing very well: Just having a conversation with an AI.\n",
      "AI:  That's great! It's always nice to converse with someone new. What brings you here today?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said \"Hi there!\"'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de94dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. What can I do for you?\n",
      "Human: I'm doing very well: Just having a conversation with an AI.\n",
      "AI:  That's great! It's always nice to converse with someone new. What brings you here today?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:  You said \"Hi there!\"\n",
      "Human: What is the alternative phrase for the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The alternative phrase for the first thing you said to me is \"Greetings!\".'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the alternative phrase for the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ed1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
